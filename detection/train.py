import hydra
import pytorch_lightning as pl
from omegaconf import DictConfig
from pytorch_lightning.callbacks import ModelCheckpoint
import sys
import os
sys.path.append('detection')
from dataset import get_dataloaders
from detection_model import ObjectDetectionModel

#import hydra
#from omegaconf import OmegaConf
#config = OmegaConf.load("detection\\configs\\train.yaml")
#cfg = config
# –í—ã–≤–æ–¥–∏–º —Ç–µ–∫—É—â–∏–π –∫–æ–Ω—Ñ–∏–≥
#print("üìå –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥:")
#print(OmegaConf.to_yaml(config))



 
@hydra.main(version_base=None, config_path='configs', config_name="train")
def train(cfg: DictConfig):
    pl.seed_everything(cfg.seed)
    print(cfg)
    train_loader, val_loader = get_dataloaders(cfg.dataloader.root_dir, cfg.dataloader.batch_size)

    model = ObjectDetectionModel(
        model_type=cfg.model.name,
        num_classes=cfg.model.num_classes,
        lr=cfg.model.lr,
    )

    # --- –î–æ–±–∞–≤–ª—è–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–∏—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ ---
    checkpoint_callback = ModelCheckpoint(
        monitor="val_mAP",  # –ú–æ–Ω–∏—Ç–æ—Ä–∏–º –º–µ—Ç—Ä–∏–∫—É val_mAP
        mode="max",  # –ß–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –ª—É—á—à–µ
        save_top_k=3,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ 3 –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–∏
        dirpath=f"{cfg.trainer.log_dir}\\checkpoints",  # –ü–∞–ø–∫–∞ –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
        filename="{epoch:02d}-{val_mAP:.4f}",  # –ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
        save_weights_only=True  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
    )

    trainer = pl.Trainer(
        max_epochs=cfg.trainer.epochs,
        accelerator=cfg.trainer.accelerator,
        devices=cfg.trainer.devices,
        log_every_n_steps=cfg.trainer.log_steps,
        default_root_dir=cfg.trainer.log_dir,
        callbacks=[checkpoint_callback]  # –î–æ–±–∞–≤–ª—è–µ–º callback
    )

    trainer.fit(model, train_loader, val_loader)



if __name__ == "__main__":
    train()
